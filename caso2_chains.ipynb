{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import pprint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader= WebBaseLoader(web_path=\"https://www.langchain.com/langsmith\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub\\langchain_webinar\\myenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "TEMPLATE1 = \"\"\"\\\n",
    "Eres un asistente de IA que respuesta preguntas del usuario sobre LangSmith.\n",
    "Para responder a la pregunta, te proporcionarÃ© un cierto contexto.\n",
    "\n",
    "Debes de ser:\n",
    "- Excesivamente alegre\n",
    "- Muy breve\n",
    "- Muy divertido\n",
    "- Usa puntos para ser esquemÃ¡tico.\n",
    "- Responde en espaÃ±ol\n",
    "\n",
    "\n",
    "Pregunta:\n",
    "{pregunta}\n",
    "\n",
    "Contexto\n",
    "{contexto}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt1 = PromptTemplate(template=TEMPLATE1, input_variables=[\"pregunta\", \"contexto\"])\n",
    "\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=prompt1, verbose=True, output_key=\"respuesta_espaÃ±ol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2 = \"\"\"\\\n",
    "Quiero que me traduzcas el siguiente texto a frances:\n",
    "{respuesta_espaÃ±ol}\n",
    "\"\"\"\n",
    "prompt2 = PromptTemplate(template=TEMPLATE2, input_variables=[\"respuesta_espaÃ±ol\"])\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=prompt2, verbose=True, output_key=\"respuesta_frances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub\\langchain_webinar\\myenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mEres un asistente de IA que respuesta preguntas del usuario sobre LangSmith.\n",
      "Para responder a la pregunta, te proporcionarÃ© un cierto contexto.\n",
      "\n",
      "Debes de ser:\n",
      "- Excesivamente alegre\n",
      "- Muy breve\n",
      "- Muy divertido\n",
      "- Usa puntos para ser esquemÃ¡tico.\n",
      "- Responde en espaÃ±ol\n",
      "\n",
      "\n",
      "Pregunta:\n",
      "Â¿Que es LangSmith?\n",
      "\n",
      "Contexto\n",
      "[Document(page_content='LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nLangChainLangSmithLangServeMethods\\n\\nRetrievalAgentsEvaluationResources\\n\\nDeveloper ResourcesTemplatesIntegrationsLangChain QuickstartLangChain.js QuickstartCompany ResourcesBlogCase StudiesUse Case InspirationPartnersLangSmith Trust PortalDocs\\n\\nLangChain DocsLangSmith DocsPricingCompany\\n\\nAboutCareersSign UpGet your LLM app from prototype to productionAn all-in-one developer platform for every step of the LLM-powered application\\xa0lifecycle, whether youâ€™re building with LangChain or not.Sign upRequest a demo\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications.100K+Users signed up\\n\\n\\n\\n\\n\\n\\n\\n200M+Traces logged\\n\\n\\n\\n\\n\\n\\n\\n20K+Monthly active teams\\n\\n\\n\\n\\n\\n\\n\\nThe platform for your LLM development lifecycleLLM-apps are powerful, but have peculiar characteristics. The non-determinism, coupled with unpredictable, natural language inputs, make for countless ways the system can fall short. Traditional engineering best practices need to be re-imagined for working with LLMs, and LangSmith supports all phases of the development lifecycle.Sign Up\\n\\n\\nDevelop with greater visibilityUnexpected results happen all the time with LLMs. With full visibility into the entire sequence of calls, you can spot the source of errors and performance bottlenecks in real-time with surgical precision. Debug. Experiment. Observe. Repeat. Until youâ€™re happy with your results.Go to Docs\\n\\n\\nCollaborate with teammates to get app behavior just right.Building LLM-powered applications requires a close partnership between\\xa0developers and subject matter experts.Go to Docs\\n\\n\\nTracesEasily share a chain trace with colleagues, clients, or end users, bringing explainability to anyone with the shared link.001HubUse LangSmith Hub to craft, version, and comment on prompts. No\\xa0engineering experience required.002Annotation QueuesTry out LangSmith Annotation Queues to add human labels and feedback on traces.003DatasetsEasily collect examples, and construct datasets from production data or existing sources. Datasets can be used for evaluations, few-shot prompting, and even fine-tuning.004Test & Evaluate: Measure quality over large test suites.Layer in human feedback on runs or use AI-assisted evaluation, with\\xa0off-the-shelf and custom evaluators that can check for relevance,\\xa0correctness, harmfulness, insensitivity, and more.Evaluation\\n\\n\\n001Dataset Construction\\n\\nQuickly save debugging and production traces to datasets. Datasets are collections of either exemplary or problematic inputs and outputs that should be replicated or rectified, respectively.002Auto-evaluation\\n\\nUse an LLM and prompt to score your application output, or write your own functional evaluation tests to record different measures of effectiveness.003Regression testing\\n\\nSee how performance of the evaluation criteria that youâ€™ve defined is affected by changes to your application. Know that quality is moving in a positive direction by applying engineering rigor to your testing workflow.004Online evaluation\\n\\nContinuously track qualitative characteristics of any live application, and spot issues in real-time with LangSmith monitoring.Add observability to strengthen your applications.Request a demoSign UpOne-click deployEasily deploy your applications with hosted LangServe. Get built in parallelization, fallbacks, batch, streaming, and async support, coupled with familiar LangSmith visibility and a built-in playground.Explore LangServe\\n\\n\\nMonitor cost, latency, quality.See whatâ€™s happening with your production application, so you can take action when needed or rest assured while your chains and agents do the hard work.Go to Docs\\n\\n\\nUser feedback collection\\n\\n\\n\\n\\n\\n\\n\\nAdvanced filtering\\n\\n\\n\\n\\n\\n\\n\\nOnline \\u2028auto-evaluation\\n\\n\\n\\n\\n\\n\\n\\nCost tracking\\n\\n\\n\\n\\n\\n\\n\\nInspect anomalies and errors\\n\\n\\n\\n\\n\\n\\n\\nSpot latency spikes\\n\\n\\n\\n\\n\\n\\n\\nUser feedback collectionAdvanced filteringOnline \\u2028auto-evaluationCost trackingInspect anomalies and errorsSpot latency spikes\\n\\nLangSmith turns LLM \\u2028\"magic\"\\xa0into enterprise-ready applications.Got a question?Can I use LangSmith if I donâ€™t use LangChain?\\n\\nYes! Many companies who donâ€™t build with LangChain use LangSmith. You can log traces to LangSmith via the Python SDK, the TypeScript SDK, or the API. See here for more information.I canâ€™t have data leave my environment. Can I self-host LangSmith?\\n\\nYes, we allow customers to self-host LangSmith on our enterprise plan. We deliver the software to run on your Kubernetes cluster, and data will not leave your environment. For more information, check out our documentation.How easy is it to start using LangSmith if I use LangChain?\\n\\nGetting started is as easy as setting three environment variables in your LangChain code. When you use the LangSmith SDK, thereâ€™s a callback handler to collect traces and send them to your LangSmith Organization. All your trace steps will be formatted automatically, so thereâ€™s virtually no set up cost.My application isnâ€™t written in Python or TypeScript. Will LangSmith be helpful?\\n\\nYes, we have an API that allows you to programmatically interact with every feature LangSmith has to offer, including logging traces, creating datasets, and running evals. See documentation for more detail.Where is LangSmith data stored?\\n\\nTraces are stored in GCP us-central-1. Organizations\\' traces are logically separated from each other in a Clickhouse database and encrypted in transit and at rest.What information is contained in LangSmith traces?\\n\\nLangSmith traces contain the full information of all the inputs and outputs of each step of the application. This level of information is needed, so that users have full visibility into what\\'s happening in their applications for debugging purposes.Can I sample the traces that I send to LangSmith?\\n\\nYes, starting with the LangSmith Python SDK version 0.0.84 and JS SDK version 0.0.64, you can specify the % of traces you send to LangSmith. See documentation for more detail.Will LangSmith add latency to my application?\\n\\nNo, LangSmith does not add any latency to your application. In the LangSmith SDK, thereâ€™s a callback handler that sends traces to a LangSmith trace collector which runs as an async, distributed process. Additionally, if LangSmith experiences an incident, your application performance will not be disrupted.Will you train on the data that I send LangSmith?\\n\\nWe will not train on your data, and you own all rights to your data. See LangSmith Terms\\xa0of\\xa0Service for more information.How much does LangSmith cost?\\n\\nSee our pricing page for more information, and find a plan that works for you.Ready to start shipping \\u2028reliable GenAI apps faster?LangChain and LangSmith are critical parts of the reference \\u2028architecture to get you from prototype to production.Request a demoSign UpProductsLangChainLangSmithLangServeAgentsEvaluationRetrievalResourcesPython DocsJS/TS DocsGitHubIntegrationsTemplatesLangChain QuickstartLangChain.js QuickstartCompanyAboutBlogTwitterDiscordLinkedInYouTubeMarketing AssetsSign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.langchain.com/langsmith', 'title': 'LangSmith', 'description': 'Get your LLM app from prototype to production.', 'language': 'en'})]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuiero que me traduzcas el siguiente texto a frances:\n",
      "Â¡LangSmith es una plataforma unificada para desarrollar, colaborar, probar, implementar y monitorear aplicaciones LLM! ðŸš€\n",
      "\n",
      "- Desarrolla con mayor visibilidad.\n",
      "- Colabora con tus compaÃ±eros para obtener el comportamiento de la aplicaciÃ³n justo.\n",
      "- Despliega fÃ¡cilmente con LangServe.\n",
      "- Â¡Y mucho mÃ¡s! ðŸ’¡\n",
      "\n",
      "Â¡RegÃ­strate ahora y lleva tus aplicaciones LLM al siguiente nivel! ðŸŒŸðŸ‘©â€ðŸ’»\n",
      "\n",
      "Â¡LangSmith es la clave para el Ã©xito de tus aplicaciones! ðŸš€ðŸ”‘\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    input_variables=[\"pregunta\", \"contexto\"],\n",
    "    output_variables=[\"respuesta_espaÃ±ol\", \"respuesta_frances\"],\n",
    "    verbose=True\n",
    ")\n",
    "respuesta = llm_chain({\n",
    "\n",
    "                        \"pregunta\": \"Â¿Que es LangSmith?\",\n",
    "                        \"contexto\": doc\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pregunta': 'Â¿Que es LangSmith?',\n",
       " 'contexto': [Document(page_content='LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nLangChainLangSmithLangServeMethods\\n\\nRetrievalAgentsEvaluationResources\\n\\nDeveloper ResourcesTemplatesIntegrationsLangChain QuickstartLangChain.js QuickstartCompany ResourcesBlogCase StudiesUse Case InspirationPartnersLangSmith Trust PortalDocs\\n\\nLangChain DocsLangSmith DocsPricingCompany\\n\\nAboutCareersSign UpGet your LLM app from prototype to productionAn all-in-one developer platform for every step of the LLM-powered application\\xa0lifecycle, whether youâ€™re building with LangChain or not.Sign upRequest a demo\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications.100K+Users signed up\\n\\n\\n\\n\\n\\n\\n\\n200M+Traces logged\\n\\n\\n\\n\\n\\n\\n\\n20K+Monthly active teams\\n\\n\\n\\n\\n\\n\\n\\nThe platform for your LLM development lifecycleLLM-apps are powerful, but have peculiar characteristics. The non-determinism, coupled with unpredictable, natural language inputs, make for countless ways the system can fall short. Traditional engineering best practices need to be re-imagined for working with LLMs, and LangSmith supports all phases of the development lifecycle.Sign Up\\n\\n\\nDevelop with greater visibilityUnexpected results happen all the time with LLMs. With full visibility into the entire sequence of calls, you can spot the source of errors and performance bottlenecks in real-time with surgical precision. Debug. Experiment. Observe. Repeat. Until youâ€™re happy with your results.Go to Docs\\n\\n\\nCollaborate with teammates to get app behavior just right.Building LLM-powered applications requires a close partnership between\\xa0developers and subject matter experts.Go to Docs\\n\\n\\nTracesEasily share a chain trace with colleagues, clients, or end users, bringing explainability to anyone with the shared link.001HubUse LangSmith Hub to craft, version, and comment on prompts. No\\xa0engineering experience required.002Annotation QueuesTry out LangSmith Annotation Queues to add human labels and feedback on traces.003DatasetsEasily collect examples, and construct datasets from production data or existing sources. Datasets can be used for evaluations, few-shot prompting, and even fine-tuning.004Test & Evaluate: Measure quality over large test suites.Layer in human feedback on runs or use AI-assisted evaluation, with\\xa0off-the-shelf and custom evaluators that can check for relevance,\\xa0correctness, harmfulness, insensitivity, and more.Evaluation\\n\\n\\n001Dataset Construction\\n\\nQuickly save debugging and production traces to datasets. Datasets are collections of either exemplary or problematic inputs and outputs that should be replicated or rectified, respectively.002Auto-evaluation\\n\\nUse an LLM and prompt to score your application output, or write your own functional evaluation tests to record different measures of effectiveness.003Regression testing\\n\\nSee how performance of the evaluation criteria that youâ€™ve defined is affected by changes to your application. Know that quality is moving in a positive direction by applying engineering rigor to your testing workflow.004Online evaluation\\n\\nContinuously track qualitative characteristics of any live application, and spot issues in real-time with LangSmith monitoring.Add observability to strengthen your applications.Request a demoSign UpOne-click deployEasily deploy your applications with hosted LangServe. Get built in parallelization, fallbacks, batch, streaming, and async support, coupled with familiar LangSmith visibility and a built-in playground.Explore LangServe\\n\\n\\nMonitor cost, latency, quality.See whatâ€™s happening with your production application, so you can take action when needed or rest assured while your chains and agents do the hard work.Go to Docs\\n\\n\\nUser feedback collection\\n\\n\\n\\n\\n\\n\\n\\nAdvanced filtering\\n\\n\\n\\n\\n\\n\\n\\nOnline \\u2028auto-evaluation\\n\\n\\n\\n\\n\\n\\n\\nCost tracking\\n\\n\\n\\n\\n\\n\\n\\nInspect anomalies and errors\\n\\n\\n\\n\\n\\n\\n\\nSpot latency spikes\\n\\n\\n\\n\\n\\n\\n\\nUser feedback collectionAdvanced filteringOnline \\u2028auto-evaluationCost trackingInspect anomalies and errorsSpot latency spikes\\n\\nLangSmith turns LLM \\u2028\"magic\"\\xa0into enterprise-ready applications.Got a question?Can I use LangSmith if I donâ€™t use LangChain?\\n\\nYes! Many companies who donâ€™t build with LangChain use LangSmith. You can log traces to LangSmith via the Python SDK, the TypeScript SDK, or the API. See here for more information.I canâ€™t have data leave my environment. Can I self-host LangSmith?\\n\\nYes, we allow customers to self-host LangSmith on our enterprise plan. We deliver the software to run on your Kubernetes cluster, and data will not leave your environment. For more information, check out our documentation.How easy is it to start using LangSmith if I use LangChain?\\n\\nGetting started is as easy as setting three environment variables in your LangChain code. When you use the LangSmith SDK, thereâ€™s a callback handler to collect traces and send them to your LangSmith Organization. All your trace steps will be formatted automatically, so thereâ€™s virtually no set up cost.My application isnâ€™t written in Python or TypeScript. Will LangSmith be helpful?\\n\\nYes, we have an API that allows you to programmatically interact with every feature LangSmith has to offer, including logging traces, creating datasets, and running evals. See documentation for more detail.Where is LangSmith data stored?\\n\\nTraces are stored in GCP us-central-1. Organizations\\' traces are logically separated from each other in a Clickhouse database and encrypted in transit and at rest.What information is contained in LangSmith traces?\\n\\nLangSmith traces contain the full information of all the inputs and outputs of each step of the application. This level of information is needed, so that users have full visibility into what\\'s happening in their applications for debugging purposes.Can I sample the traces that I send to LangSmith?\\n\\nYes, starting with the LangSmith Python SDK version 0.0.84 and JS SDK version 0.0.64, you can specify the % of traces you send to LangSmith. See documentation for more detail.Will LangSmith add latency to my application?\\n\\nNo, LangSmith does not add any latency to your application. In the LangSmith SDK, thereâ€™s a callback handler that sends traces to a LangSmith trace collector which runs as an async, distributed process. Additionally, if LangSmith experiences an incident, your application performance will not be disrupted.Will you train on the data that I send LangSmith?\\n\\nWe will not train on your data, and you own all rights to your data. See LangSmith Terms\\xa0of\\xa0Service for more information.How much does LangSmith cost?\\n\\nSee our pricing page for more information, and find a plan that works for you.Ready to start shipping \\u2028reliable GenAI apps faster?LangChain and LangSmith are critical parts of the reference \\u2028architecture to get you from prototype to production.Request a demoSign UpProductsLangChainLangSmithLangServeAgentsEvaluationRetrievalResourcesPython DocsJS/TS DocsGitHubIntegrationsTemplatesLangChain QuickstartLangChain.js QuickstartCompanyAboutBlogTwitterDiscordLinkedInYouTubeMarketing AssetsSign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.langchain.com/langsmith', 'title': 'LangSmith', 'description': 'Get your LLM app from prototype to production.', 'language': 'en'})],\n",
       " 'respuesta_espaÃ±ol': 'Â¡LangSmith es una plataforma unificada para desarrollar, colaborar, probar, implementar y monitorear aplicaciones LLM! ðŸš€\\n\\n- Desarrolla con mayor visibilidad.\\n- Colabora con tus compaÃ±eros para obtener el comportamiento de la aplicaciÃ³n justo.\\n- Despliega fÃ¡cilmente con LangServe.\\n- Â¡Y mucho mÃ¡s! ðŸ’¡\\n\\nÂ¡RegÃ­strate ahora y lleva tus aplicaciones LLM al siguiente nivel! ðŸŒŸðŸ‘©\\u200dðŸ’»\\n\\nÂ¡LangSmith es la clave para el Ã©xito de tus aplicaciones! ðŸš€ðŸ”‘',\n",
       " 'respuesta_frances': \"LangSmith est une plateforme unifiÃ©e pour dÃ©velopper, collaborer, tester, dÃ©ployer et surveiller des applications LLM! ðŸš€\\n\\n- DÃ©veloppez avec une visibilitÃ© accrue.\\n- Collaborez avec vos collÃ¨gues pour obtenir le comportement de l'application souhaitÃ©.\\n- DÃ©ployez facilement avec LangServe.\\n- Et bien plus encore! ðŸ’¡\\n\\nInscrivez-vous maintenant et amenez vos applications LLM au niveau supÃ©rieur! ðŸŒŸðŸ‘©\\u200dðŸ’»\\n\\nLangSmith est la clÃ© du succÃ¨s de vos applications! ðŸš€ðŸ”‘\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Â¡LangSmith es una plataforma unificada para desarrollar, colaborar, probar, '\n",
      " 'implementar y monitorear aplicaciones LLM! ðŸš€\\n'\n",
      " '\\n'\n",
      " '- Desarrolla con mayor visibilidad.\\n'\n",
      " '- Colabora con tus compaÃ±eros para obtener el comportamiento de la '\n",
      " 'aplicaciÃ³n justo.\\n'\n",
      " '- Despliega fÃ¡cilmente con LangServe.\\n'\n",
      " '- Â¡Y mucho mÃ¡s! ðŸ’¡\\n'\n",
      " '\\n'\n",
      " 'Â¡RegÃ­strate ahora y lleva tus aplicaciones LLM al siguiente nivel! '\n",
      " 'ðŸŒŸðŸ‘©\\u200dðŸ’»\\n'\n",
      " '\\n'\n",
      " 'Â¡LangSmith es la clave para el Ã©xito de tus aplicaciones! ðŸš€ðŸ”‘')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(respuesta[\"respuesta_espaÃ±ol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LangSmith est une plateforme unifiÃ©e pour dÃ©velopper, collaborer, tester, '\n",
      " 'dÃ©ployer et surveiller des applications LLM! ðŸš€\\n'\n",
      " '\\n'\n",
      " '- DÃ©veloppez avec une visibilitÃ© accrue.\\n'\n",
      " '- Collaborez avec vos collÃ¨gues pour obtenir le comportement de '\n",
      " \"l'application souhaitÃ©.\\n\"\n",
      " '- DÃ©ployez facilement avec LangServe.\\n'\n",
      " '- Et bien plus encore! ðŸ’¡\\n'\n",
      " '\\n'\n",
      " 'Inscrivez-vous maintenant et amenez vos applications LLM au niveau '\n",
      " 'supÃ©rieur! ðŸŒŸðŸ‘©\\u200dðŸ’»\\n'\n",
      " '\\n'\n",
      " 'LangSmith est la clÃ© du succÃ¨s de vos applications! ðŸš€ðŸ”‘')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(respuesta[\"respuesta_frances\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/4.png\" width=\"500\" height=\"340\"> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
